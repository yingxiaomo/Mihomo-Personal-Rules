import argparse
import os
import re
import sys
import glob
import time
from pathlib import Path

import joblib
import lightgbm as lgb
import numpy as np
import pandas as pd
import requests
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler, StandardScaler

SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent

DEFAULT_DATA_DIR = PROJECT_ROOT / "data"
DEFAULT_MODEL_PATH = PROJECT_ROOT / "models" / "Model.bin"
CACHE_DIR = SCRIPT_DIR / "cache"
GO_SOURCE_CACHE_PATH = CACHE_DIR / "transform.go.cache"

GO_SOURCE_URL = "https://raw.githubusercontent.com/vernesong/mihomo/Alpha/component/smart/lightgbm/transform.go"

BIASED_FEATURES = [
    'download_mb', 'upload_mb', 'traffic_density', 'traffic_ratio', 
    'last_used_seconds', 'duration_minutes', 
    'history_download_mb', 'history_upload_mb'
]

COMPLEX_FEATURES = [
    'asn_feature', 'country_feature', 'address_feature', 
    'port_feature', 'connection_type_feature'
]

CONTINUOUS_FEATURES = [
    'connect_time', 'latency', 
    'upload_mb', 'download_mb', 
    'maxuploadrate_kb', 'maxdownloadrate_kb',
    'history_upload_mb', 'history_download_mb',
    'history_maxuploadrate_kb', 'history_maxdownloadrate_kb',
    'duration_minutes', 'last_used_seconds',
    'traffic_density', 'traffic_ratio',
    'asn_hash', 'host_hash', 'ip_hash', 'geoip_hash'
]

COUNT_FEATURES = ['success', 'failure'] 

LGBM_PARAMS = {
    'objective': 'regression',
    'metric': 'mae',
    'boosting_type': 'gbdt',
    'n_estimators': 1000,
    'learning_rate': 0.05,
    'num_leaves': 31,
    'max_depth': -1,
    'min_child_samples': 20,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'random_state': 42,
    'n_jobs': -1,
    'verbosity': -1
}

def print_separator(title=None):
    if title:
        print("=" * 60)
        print(f"{title}")
        print("=" * 60)
    else:
        print("=" * 60)

def fetch_go_source():
    print("\n[æ­¥éª¤1] Go æºç è§£æ")
    
    content = ""
    if GO_SOURCE_CACHE_PATH.exists():
        if (time.time() - GO_SOURCE_CACHE_PATH.stat().st_mtime) < 86400:
            print(f"æˆåŠŸåŠ è½½æœ¬åœ°ç¼“å­˜: {GO_SOURCE_CACHE_PATH}")
            return GO_SOURCE_CACHE_PATH.read_text(encoding='utf-8')

    print(f"æ­£åœ¨ä¸‹è½½ Go æºæ–‡ä»¶: {GO_SOURCE_URL}")
    try:
        CACHE_DIR.mkdir(exist_ok=True)
        response = requests.get(GO_SOURCE_URL, timeout=10)
        response.raise_for_status()
        content = response.text
        GO_SOURCE_CACHE_PATH.write_text(content, encoding='utf-8')
        print("ä¸‹è½½å¹¶ç¼“å­˜æˆåŠŸ")
        return content
    except Exception as e:
        if GO_SOURCE_CACHE_PATH.exists():
            print(f"ä¸‹è½½å¤±è´¥ ({e})ï¼Œä½¿ç”¨æ—§ç¼“å­˜")
            return GO_SOURCE_CACHE_PATH.read_text(encoding='utf-8')
        raise RuntimeError(f"æ— æ³•è·å– Go æºç : {e}")

def parse_feature_order(go_content):
    print("å¼€å§‹è§£æ getDefaultFeatureOrder å‡½æ•°...")
    func_match = re.search(r'func getDefaultFeatureOrder\(\) map\[int\]string \{(.*?)\}', go_content, re.DOTALL)
    if not func_match:
        print("è­¦å‘Š: æ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œä½¿ç”¨åå¤‡ç‰¹å¾åˆ—è¡¨")
        return get_fallback_features()

    feature_map = {}
    pairs = re.findall(r'(\d+):\s*"([^"]+)"', func_match.group(1))
    for idx, name in pairs:
        feature_map[int(idx)] = name
    
    if not feature_map:
        return get_fallback_features()
    
    print(f"æˆåŠŸè§£æ {len(feature_map)} ä¸ªç‰¹å¾çš„é¡ºåºå®šä¹‰")
    print(f"ç‰¹å¾é¡ºåºè§£æå®Œæˆï¼Œå…± {len(feature_map)} ä¸ªç‰¹å¾")
    return feature_map

def get_fallback_features():
    features = [
        'success', 'failure', 'connect_time', 'latency', 'upload_mb', 'download_mb', 
        'duration_minutes', 'last_used_seconds', 'is_udp', 'is_tcp', 'asn_feature', 
        'country_feature', 'address_feature', 'port_feature', 'traffic_ratio', 
        'traffic_density', 'connection_type_feature', 'asn_hash', 'host_hash', 
        'ip_hash', 'geoip_hash'
    ]
    return {i: f for i, f in enumerate(features)}

def load_data(data_dir, days=15):
    print("\n[æ­¥éª¤2] æ•°æ®åŠ è½½ä¸æ¸…æ´—")
    print(f"å¼€å§‹ä»æ•°æ®ç›®å½•åŠ è½½æ‰€æœ‰ CSV æ–‡ä»¶: {data_dir}")
    
    if not data_dir.exists():
        print(f"é”™è¯¯: ç›®å½•ä¸å­˜åœ¨ {data_dir}")
        raise FileNotFoundError(f"Data directory not found: {data_dir}")

    all_files = glob.glob(str(data_dir / "*.csv"))
    
    cutoff_time = time.time() - (days * 86400)
    recent_files = [f for f in all_files if os.path.getmtime(f) > cutoff_time]
    
    if not recent_files:
        print("è­¦å‘Š: æœªå‘ç°è¿‘æœŸæ•°æ®ï¼Œå°è¯•åŠ è½½æ‰€æœ‰æ–‡ä»¶...")
        recent_files = all_files
        
    if not recent_files:
        raise FileNotFoundError("æ²¡æœ‰æ‰¾åˆ° CSV æ–‡ä»¶")

    print(f"--- æ‰¾åˆ° {len(recent_files)} ä¸ªæ•°æ®æ–‡ä»¶ ---")
    
    dfs = []
    for f in recent_files:
        fname = os.path.basename(f)
        print(f"å°è¯•åŠ è½½æ–‡ä»¶: {fname}...")
        try:
            df = pd.read_csv(f, encoding='utf-8', on_bad_lines='skip')
            print(f"æ–‡ä»¶å¤„ç†å®Œæˆ: {len(df)} æ¡è®°å½•")
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(f, encoding='latin-1', on_bad_lines='skip')
                print(f"è­¦å‘Š: æ–‡ä»¶ '{fname}' ä½¿ç”¨ latin-1 ç¼–ç æˆåŠŸåŠ è½½: {len(df)} æ¡è®°å½•")
            except:
                print(f"è·³è¿‡æ— æ³•è¯»å–çš„æ–‡ä»¶: {fname}")
                continue
        
        age_days = (time.time() - os.path.getmtime(f)) / 86400
        df['__file_age_days'] = age_days
        dfs.append(df)
    
    if not dfs:
        raise ValueError("æ— æ³•åŠ è½½ä»»ä½•æ•°æ®")
    
    print("\nåˆå¹¶æ‰€æœ‰æ•°æ®æ–‡ä»¶...")
    merged_df = pd.concat(dfs, ignore_index=True)
    print(f"æ•°æ®åˆå¹¶å®Œæˆï¼Œæ€»è®°å½•æ•°: {len(merged_df)}")
    return merged_df

def preprocess_data(df, feature_order):
    print("\n[æ­¥éª¤3] ç‰¹å¾æå–")
    print("å¼€å§‹æ„å»ºç‰¹å¾çŸ©é˜µå’Œç›®æ ‡å˜é‡...")

    TARGET_MAIN = 'maxdownloadrate_kb'
    if TARGET_MAIN in df.columns:
        df[TARGET_MAIN] = pd.to_numeric(df[TARGET_MAIN], errors='coerce').fillna(0)
    else:
        TARGET_MAIN = 'download_mbps' if 'download_mbps' in df.columns else None

    if not TARGET_MAIN:
        raise ValueError("ä¸¥é‡é”™è¯¯: æœªæ‰¾åˆ°ç›®æ ‡åˆ— (maxdownloadrate_kb)")

    original_rows = len(df)
    high_quality_df = df[df[TARGET_MAIN] > 0.1].copy()
    
    use_weight_as_fallback = False
    
    if len(high_quality_df) > 100:
        df = high_quality_df
        y = df[TARGET_MAIN]
        print(f"æ•°æ®æ¸…æ´—: {original_rows} -> {len(df)} æ¡è®°å½• (ä¿ç•™çœŸå®æµ‹é€Ÿæ•°æ®)")
    else:
        print(f"è­¦å‘Š: æœ‰æ•ˆæµ‹é€Ÿæ•°æ®æå°‘ ({len(high_quality_df)})ï¼Œå¯ç”¨å…œåº•æ¨¡å¼")
        if 'weight' in df.columns:
            y = df['weight']
            use_weight_as_fallback = True
        else:
            y = df[TARGET_MAIN]

    if 'latency' in df.columns:
        df['latency_stability'] = df['latency'] / (df['latency'] + 1e-6)
    
    mask_features = BIASED_FEATURES + COMPLEX_FEATURES
    for col in mask_features:
        if col in df.columns:
            df[col] = 0.0

    ordered_features = [feature_order[i] for i in sorted(feature_order.keys())]
    valid_cols = [col for col in ordered_features if col in df.columns]
    X = df[valid_cols]
    X = X.select_dtypes(include=np.number)

    print(f"ç‰¹å¾æå–å®Œæˆ - ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}, ç›®æ ‡å˜é‡å½¢çŠ¶: {y.shape}")

    print("\n[æ­¥éª¤4] ç‰¹å¾æ ‡å‡†åŒ–")
    print("å¼€å§‹ç‰¹å¾æ ‡å‡†åŒ–å¤„ç†...")
    
    scalers = {}
    if CONTINUOUS_FEATURES:
        continuous_present = [c for c in CONTINUOUS_FEATURES if c in X.columns]
        if continuous_present:
            scaler_std = StandardScaler()
            X[continuous_present] = scaler_std.fit_transform(X[continuous_present])
            scalers['standard'] = scaler_std
            scalers['std_features'] = continuous_present
            print(f"StandardScaler å¤„ç†å®Œæˆï¼Œå½±å“ç‰¹å¾æ•°: {len(continuous_present)}")

    if COUNT_FEATURES:
        count_present = [c for c in COUNT_FEATURES if c in X.columns]
        if count_present:
            scaler_robust = RobustScaler()
            X[count_present] = scaler_robust.fit_transform(X[count_present])
            scalers['robust'] = scaler_robust
            scalers['rob_features'] = count_present
            print(f"RobustScaler å¤„ç†å®Œæˆï¼Œå½±å“ç‰¹å¾æ•°: {len(count_present)}")

    if '__file_age_days' in df.columns:
        base_weight = 1.0 / (1.0 + 0.1 * df['__file_age_days'])
        if not use_weight_as_fallback:
            speed_bonus = np.log1p(df[TARGET_MAIN]) * 0.05
            df['sample_weight'] = base_weight + speed_bonus
        else:
            df['sample_weight'] = base_weight
    else:
        df['sample_weight'] = 1.0
        
    sample_weights = df['sample_weight']

    return X, y, sample_weights, scalers

def save_model_and_params(model, scalers, feature_order, output_path):
    print("\n[æ­¥éª¤7] æ¨¡å‹ä¿å­˜")
    print(f"å¼€å§‹ä¿å­˜æ¨¡å‹è‡³: {output_path}")
    
    joblib.dump(model, output_path)
    
    feature_name_to_idx = {v: k for k, v in feature_order.items()}

    ini_string = "\n\n[transforms]\n"
    definitions_string = "\n[definitions]\n"
    order_string = "\n[order]\n"
    
    scaler_std = scalers.get('standard')
    std_feature_names = scalers.get('std_features', [])

    if scaler_std and std_feature_names:
        feature_indices = []
        valid_indices = []
        for i, name in enumerate(std_feature_names):
            if name in feature_name_to_idx:
                feature_indices.append(str(feature_name_to_idx[name]))
                valid_indices.append(i)
        
        if feature_indices:
            means = scaler_std.mean_[valid_indices]
            scales = scaler_std.scale_[valid_indices]
            definitions_string += "std_type=StandardScaler\n"
            definitions_string += "std_features=" + ",".join(feature_indices) + "\n"
            definitions_string += "std_mean=" + ",".join(f"{x:.6f}" for x in means) + "\n"
            definitions_string += "std_scale=" + ",".join(f"{x:.6f}" for x in scales) + "\n"

    scaler_rob = scalers.get('robust')
    rob_feature_names = scalers.get('rob_features', []) 

    if scaler_rob and rob_feature_names:
        feature_indices = []
        valid_indices = []
        for i, name in enumerate(rob_feature_names):
            if name in feature_name_to_idx:
                feature_indices.append(str(feature_name_to_idx[name]))
                valid_indices.append(i)
        
        if feature_indices:
            centers = scaler_rob.center_[valid_indices]
            scales = scaler_rob.scale_[valid_indices]
            definitions_string += "\nrobust_type=RobustScaler\n"
            definitions_string += "robust_features=" + ",".join(feature_indices) + "\n"
            definitions_string += "robust_center=" + ",".join(f"{x:.6f}" for x in centers) + "\n"
            definitions_string += "robust_scale=" + ",".join(f"{x:.6f}" for x in scales) + "\n"

    for i in sorted(feature_order.keys()):
        order_string += f"{i} = {feature_order[i]}\n"

    final_config = ini_string + order_string + definitions_string + "transform=true\n[/transforms]"

    with open(output_path, "ab") as f:
        f.write(final_config.encode('utf-8'))
    
    print("æ¨¡å‹ä¿å­˜æˆåŠŸï¼Œå¯ä»¥ç›´æ¥éƒ¨ç½²")

def training_logger_cn(period=100):
    def _callback(env):
        if period > 0 and (env.iteration + 1) % period == 0:
            msg_parts = []
            for data_name, eval_name, result, *rest in env.evaluation_result_list:
                if data_name == 'valid_0': d_name = 'éªŒè¯é›†'
                elif data_name == 'train': d_name = 'è®­ç»ƒé›†'
                else: d_name = data_name
                
                if eval_name == 'l1': e_name = 'MAEè¯¯å·®'
                elif eval_name == 'l2': e_name = 'MSEè¯¯å·®'
                else: e_name = eval_name
                
                msg_parts.append(f"{d_name} {e_name}: {result:.6f}")
            print(f"[è¿­ä»£ {env.iteration + 1:4d}] " + "  ".join(msg_parts))
    _callback.order = 10
    return _callback

def main():
    print_separator("Mihomo æ™ºèƒ½æƒé‡æ¨¡å‹è®­ç»ƒ")
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_dir", type=Path, default=DEFAULT_DATA_DIR)
    parser.add_argument("--output", "--output-file", dest="output", type=Path, default=DEFAULT_MODEL_PATH)
    args = parser.parse_args()

    try:
        go_content = fetch_go_source()
        feature_order = parse_feature_order(go_content)
    except Exception as e:
        print(f"é”™è¯¯: åŒæ­¥ Go æºç å¤±è´¥: {e}")
        sys.exit(1)

    try:
        df = load_data(args.data_dir, days=15)
    except Exception as e:
        print(f"é”™è¯¯: åŠ è½½æ•°æ®å¤±è´¥: {e}")
        sys.exit(1)

    try:
        X, y, weights, scalers = preprocess_data(df, feature_order)
    except Exception as e:
        print(f"é”™è¯¯: é¢„å¤„ç†å¤±è´¥: {e}")
        sys.exit(1)

    print("\n[æ­¥éª¤5] è®­ç»ƒæµ‹è¯•é›†åˆ’åˆ†")
    X_train, X_val, y_train, y_val, w_train, w_val = train_test_split(
        X, y, weights, test_size=0.2, random_state=42
    )
    print(f"æ•°æ®åˆ’åˆ†å®Œæˆ - è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_val.shape}")

    print("\n[æ­¥éª¤6] æ¨¡å‹è®­ç»ƒ")
    print("å¼€å§‹ LightGBM æ¨¡å‹è®­ç»ƒ...")
    model = lgb.LGBMRegressor(**LGBM_PARAMS)
    
    callbacks = [
        lgb.early_stopping(stopping_rounds=50, verbose=False),
        training_logger_cn(period=100)
    ]

    model.fit(
        X_train, y_train,
        sample_weight=w_train,
        eval_set=[(X_val, y_val)],
        eval_sample_weight=[w_val],
        callbacks=callbacks
    )
    print("æ¨¡å‹è®­ç»ƒå®Œæˆ")

    if model.best_iteration_ == LGBM_PARAMS['n_estimators']:
         print(f"è®­ç»ƒçŠ¶æ€: æœªè§¦å‘æ—©åœã€‚æœ€ä½³è¿­ä»£è½®æ•°: [{model.best_iteration_}]")
    else:
         print(f"è®­ç»ƒçŠ¶æ€: è§¦å‘æ—©åœã€‚æœ€ä½³è¿­ä»£è½®æ•°: [{model.best_iteration_}]")

    predictions = model.predict(X_val)
    mae = mean_absolute_error(y_val, predictions)
    r2 = r2_score(y_val, predictions)
    
    final_score = max(0, r2 * 10)

    print(f"æµ‹è¯•é›† MAEè¯¯å·®: {mae:.4f}")
    print(f"æ¨¡å‹æœ€ç»ˆè¯„åˆ†: {final_score:.3f} / 10.0")
    
    if final_score > 9.5:
        print("âœ¨ è¯„çº§: Sçº§ - æä½³ (è§„å¾‹æå¼ºï¼Œæ•°æ®è´¨é‡å®Œç¾)")
    elif final_score > 8.0:
        print("ğŸŸ¢ è¯„çº§: Açº§ - è‰¯å¥½ (æ¨¡å‹å¯ç”¨æ€§é«˜)")
    elif final_score > 6.0:
        print("ğŸŸ¡ è¯„çº§: Bçº§ - åŠæ ¼ (éƒ¨åˆ†æ•°æ®å¯èƒ½å­˜åœ¨å¹²æ‰°)")
    elif final_score > 4.0:
        print("ğŸŸ  è¯„çº§: Cçº§ - ä¸€èˆ¬ (ç‰¹å¾å…³è”åº¦å¼±)")
    else:
        print("ğŸ”´ è¯„çº§: Dçº§ - ä¸åˆæ ¼ (æ•°æ®ä¸¥é‡ä¸è¶³æˆ–å™ªå£°è¿‡å¤§)")

    args.output.parent.mkdir(parents=True, exist_ok=True)
    save_model_and_params(model, scalers, feature_order, args.output)

    print_separator()
    print("æ¨¡å‹è®­ç»ƒæµç¨‹å®Œæˆ")
    print(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
    print("æ¨¡å‹å¯è¿›è¡Œç”Ÿäº§ç¯å¢ƒéƒ¨ç½²")
    print_separator()

if __name__ == "__main__":
    main()